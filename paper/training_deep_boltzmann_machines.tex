\documentclass{article} % For LaTeX2e
% \usepackage{nips13submit_e,times}
\usepackage{times}
\usepackage{fullpage}
\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{verbatim}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09


\title{Training Deep Boltzmann Machines}


\author{
Mark Stoehr %% \thanks{ Use footnote for providing further information
%% about author (webpage, alternative address)---\emph{not} for acknowledging
%% funding agencies.}
\\
Department of Computer Science\\
University of Chicago\\
Chicago, IL 60637 \\
\texttt{stoehr@cs.uchicago.edu} \\
%% \And
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%% \AND
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%% \And
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%% \And
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%% (if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\sign}{\mathop{\mathrm{sign}}}
%\newcommand{\diag}{\mathop{\mathrm{diag}}}
%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
This short note describes the objective function and training strategy for Deep Boltzmann Machines (DBMs)
and other models that I am working with in my current research project.
\end{abstract}

\section{Introduction}

The Deep Boltzmann Machine (DBM) is an undirected graphical model that
can be used to perform unsupervised deep learning. It is unique among deep learning approaches
because it is a statistical model, can incorporate multiple modalities, and it incorporates both feed-forward
and feed-back connections. Our principal conern of this paper is the concrete implementations of DBMs which we describe using properties of a particular
implementation of a DBM in Python and CUDA which ultimately is a collection of typed bits residing on physical hardware.  
Properties of concrete object are, as discussed in \cite[Chapter~1]{stepanov2009elements},
are correspondences
to abstract entities which in our case are the DBM's corresponding exponential family statistical
model description, graphical model formalism, and deep neural network characterization. The relationship between the DBM and its abstract
formulations are useful to us because 
they may be used to derive and mathematically justify the learning procedures we use.  The abstract 
specification is by nature hardware-independent and is specified in terms of mathematical objects and allows us to make use of 
statistical theory. Such descriptions have been worked out largely by other authors and our contribution is to formulate it
in such a way that it will be easy to show the correspondences between the abstract and the concrete.  In particular, we wish to show
how the abstract DBM theory imposes specific syntactic and semantic constraints upon DBM implementations. 

A DBM or other learning method may be viewed as a compiler for data.
We mean this in the sense that it can produce machine code from data.
The machine code in question is the bit-representation of the DBM
state after using the learning procedures available to the DBM with
the data.  The requirements on a learning algorithm, and, in
particular, a deep learning algorithm are given in terms of learning
and inferential procedures. These are:
\begin{enumerate}
\item inference of latent variables given partially
observed data 
\item construction of feature functions
\item estimation of model parameters to improve the inferential
  procedures.
\end{enumerate}  
We will describe the abstract properties satisfied by any DBM-based
procedure achieving those three goals. 

Our approach to the three problems is based on approximating maximum
likelihood and {\it maximum a posteriori} (MAP) assignment.  To
develop our approximations we need to specify a likelihood function
for the DBM.  DBMs are exponential family statistical models with a
real-valued parameter vector (estimated during training) and a family
of fixed feature functions called the sufficient statistics.  DBMs
also belong to the family of deep learning methods: the variables may
be partitioned into mutually exclusive groups called {\it layers} that
analagously correspond to the layers in a multilayer perceptron. Even
with this grouping and understanding the structure inference
(i.e. predicting the label or the hidden state of the DBM for given
input data) and training (estimating the parameters) are both
intractable.  We will show how these two characterizations of the DBM allow us to specify the likelihood.

Approximate MAP inference is necessary and the mean-field approximation gives rise to an interesting inference algorithm
which may be interpreted as a recurrent neural network.  The recurrence is over hidden layers and features a natural
bidirectionality.  Other papers have sketched out how such a recurrent neural network may be trained using discriminative
criteria (called multiprediction DBMs), and in this work we describe more of the algorithmic details and their
mathematical derivation.  Building on this understanding we describe a novel bi-direction long term short term memory
DBM whose mean-field approximation gives rise to an alternative LSTM.

We approximate maximum likelihood estimation of the parameters using a persistant contrastive
divergence (PCD) algorithm. Due to the successes obtained by deep learning in real-valued signal
tasks such as speech recognition and machine vision this tutorial will
develop a DBM that can perform classification on real-valued data and we will describe PCD specifically suited for that
particular network.  We additionally describe the principles for a CUDA implementation
that allows for faster computations.  The details of which have not been discussed elsewhere.  Other approaches are possible based on in-painting
as in the MP-DBM and we relegate that to a future paper.

In summary, the contributions of this work are
\begin{enumerate}
\item Algorithmic details and mathematical derivations for a DBM with Gaussian and soft-max visible layers
\item MP-DBM training and inference algorithms
\item Algorithmic details for the PCD training algorithm on CUDA
\end{enumerate}



\subsection{Notation}

We introduce some notation to simplify the presentation. Let
$[k]=\{0,1,\ldots,k-1\}$.  When indexing elements of a set we will
typically use the set $[k]$ or zero-based indexing to keep with common
programming conventions.  We use bold-face to denote vectors where
$\mathbf{v}\in\mathbb{R}^D$ has $D$ entries each belonging to the real
field.  To refer to components of a vector we drop the boldface and
use a subscript so that $v_j$ is the $j$th entry of $\mathbf{v}$.  We
will index different vectors with superscripts usually so that
$\mathbf{v}^0$ and $\mathbf{v}^1$ refer to separate vectors. To refer
to matrices we use typeface: e.g. $\mathtt{A}$. Visible layers will
usually be denoted with $\mathbf{v}^{(j)}$ and hidden layers will
usually be denoted as $\mathbf{h}^{(j)}$, and a general layer (not
specified as observed or latent) will be written $\mathbf{u}^{(j)}$.
We refer to a sequence of multiple layers as $\overline{\mathbf{u}}$.
The sufficient statistics or feature maps will be denoted as $\boldsymbol{\phi}^{(j)}$
for layer $\mathbf{u}^{(j)}$.  In general we will denote single coordinate functions of the
feature map using greek letters such as $\phi^{(j)}_i$ and we use bold greek letters
$\boldsymbol{\phi}^{(j)}$ for
functions that map to multiple coordinates of the feature vector. Similar to the rule
for sequences of layers the complete feature vector over all layers is denoted $\overline{\boldsymbol{\phi}}$.


The energy functions in DBMs make heavy use of inner products but we
will follow the standard convention in the Boltzmann Machine (BM)
 literature of writing inner products using
summations.  This allows us to be very careful and particular about
what is being summed over.

\subsection{Statistical Setting}

The most fundamental object of interest is the data which are usually vectors $\mathbf{v}\in\mathbb{R}^{F_v}$ paired
with categorical labels $c\in [D]$.  In {\it supervised learning} there is a {\it training phase } where input data
are presented as pairs $(\mathbf{v},c)$ and during the {\it testing phase} we only have access to the real vectors
$\mathbf{v}$ and must guess the label $c$.  The supervised learning problem is the basic inferential problem and we solve it
with a DBM using the DBM likelihood function $\mathbb{P}(\cdot ; \overline{\boldsymbol{\psi}})$ which gives a joint likelihood
over the vector-label pair: $\mathbb{P}(\mathbf{v},c ; \overline{\boldsymbol{\psi}})$.  We can perform inference in the testing
phase via MAP inference:
\begin{align}
\hat{c} &= \max_c \mathbb{P}(c\mid \mathbf{v}; \overline{\boldsymbol{\psi}}) \notag\\
& = \max_c \mathbb{P}(c\mid \mathbf{v}; \overline{\boldsymbol{\psi}}) \mathbb{P}(\mathbf{v}; \overline{\boldsymbol{\psi}}) \notag\\
& = \max_c \mathbb{P}(c, \mathbf{v}; \overline{\boldsymbol{\psi}}) 
\end{align}

Maximum likelihood is a method estimate parameters in the supervised setting or the unsupervised setting (where the training data has no labels).
For maximum likelihood we solve the problem
\begin{equation}
\max_{\overline{\boldsymbol{\psi}}} \sum_{n=0}^{N-1}\log \mathbb{P}(\mathbf{v}_n, c_n;\overline{\boldsymbol{\psi}})
\end{equation}
in the supervised setting where we have training data $\{ \mathbf{v}_n, c_n\}_{n=0}^{N-1}$.  If training is unsupervised then our estimation
procedure solves
\begin{equation}
\max_{\overline{\boldsymbol{\psi}}} \sum_{n=0}^{N-1} \log \sum_{c=0}^{D-1}\mathbb{P}(\mathbf{v}_n, c;\overline{\boldsymbol{\psi}})
\end{equation}
where we have marginalized the distribution over the labels.

The third problem that we address in this paper is feature extraction
using a deep network.  Given a parameter estimate
$\overline{\boldsymbol{\psi}}$ learned using a maximum likelihood
procedure we can extract features by means of hidden variables.  In
addition to the real-vector $\mathbf{v}$ and categorical label $c$ the
DBM also has several sets of hidden random variables
$\mathbf{h}^{(0)}$, $\mathbf{h}^{(1)}$, $\ldots$: each a binary
vector. We may concatenate all these groups of binary random variables
into a single supervector $\mathbf{h}$.  We may then rewrite the
likelihood function for the DBM over the supervised data
$(\mathbf{v},c)$ as
\begin{equation}
\sum_{\mathbf{h}}\mathbb{P}(\mathbf{v},c, \mathbf{h} ; \overline{\boldsymbol{\psi}})
\end{equation}
so that we marginalize over all possible settings of the hidden variables ($2^{\dim \mathbf{h}}$ states).  We may use this
to define a conditional distribution over the hidden states given the training data:
\begin{equation}
\mathbb{P}(\mathbf{h}\mid \mathbf{v},c ; \overline{\boldsymbol{\psi}}) = \frac{\mathbb{P}(\mathbf{v},c, \mathbf{h} ; \overline{\boldsymbol{\psi}})}{\sum_{\mathbf{h}} \mathbb{P}(\mathbf{v},c, \mathbf{h} ; \overline{\boldsymbol{\psi}})}.
\end{equation}
One difficulty of using this conditional distribution is that it does not, in general, belong to the exponential family so it does not
have a set of sufficient statistics that summarizes the statistical information in a fixed-dimensional summary.
So, to construct the feature map we use an approximating exponential family distribution $\mathbb{Q}(\mathbf{h}; \overline{\boldsymbol{\mu}})$
and we estimate the parameters $\overline{\boldsymbol{\mu}}$ to minimize the Kullback-Leibler divergence
\begin{equation}
\min_{\overline{\boldsymbol{\mu}}} D_{KL}(\mathbb{Q}(\mathbf{h}; \overline{\boldsymbol{\mu}}) \| \mathbb{P}(\mathbf{h}\mid \mathbf{v},c ; \overline{\boldsymbol{\psi}})).
\end{equation}
The estimated parameters $\overline{\boldsymbol{\mu}}$ provide a feature map for the training pair $(\mathbf{v},c)$ since
the parameters compact summarize of the latent information contained in the DBM.  To see why, note that the moments of the 
sufficient statistics can be computed via differentiating the partition function (this follows from the form of the cumulant generating function).

Since we will be making use of mean-field approximations we will be
using factorial distributions.  Given a vector of variables
$\mathbf{u}$ of dimension $F$ a factorial distribution $\mathbb{Q}(\cdot;\overline{\boldsymbol{\mu}})$ over $\mathbf{u}$ has a log-likelihood
\begin{equation}
\log \mathbb{Q}(\mathbf{u};\boldsymbol{\mu}) = \sum_{i=0}^{F-1} \boldsymbol{\phi}_i(u_i)^\top \boldsymbol{\mu}_i - A_i(\boldsymbol{\mu}_i)
\end{equation}
where $\overline{\boldsymbol{\mu}}=(\boldsymbol{\mu}_0,\ldots,\boldsymbol{\mu}_{F-1})^\top$ is the parameter vector and
the log-partition function $A$ may be decomposed as
\begin{equation}
A(\overline{\boldsymbol{\mu}}) = \sum_{i=0}^{F-1} A_i(\boldsymbol{\mu}_i).
\end{equation}
Observe that factorial distributions are defined entirely by their
marginals and all variables are treated as independent.  This
separability will be key in deriving efficient algorithms.  If
$\mathbf{u}\in \{0,1\}^F$ the factorial distribution is the product of Bernoulli random variables model
\begin{equation}
\log \mathbb{Q}(\mathbf{u};\boldsymbol{\mu}) = \sum_{i=0}^{F-1} u_i\log\frac{\mu_i}{1-\mu_i} + \log(1-\mu_i)
\end{equation}
where each parameter $\mu_i$ is the approximate marginal for the variable $u_i$.



\section{Model Specification}

A specification of the DBM must include either an exact or approximate likelihood function.  Due to the intractability of properly
normalizing the DBM likelihood function we opt to work with the energy function or the unnormalized log likelihood rather than the full log likelihood
function. When the hidden binary variables are observed the DBM is an exponential family model so the energy function is simply an inner product between a feature map known as the sufficient statistics and the vector of DBM parameters.
  A taxonomy of parameters and variables will be specified in terms of their mathematical domains using a conceptual basis borrowed from
the deep learning literature.

The following subsection describes the programmatic implementation of the constructor, assignment, destructor, initialization, equality,
and underlying type.  These will establish our DBM type as { \it regular} in the sense of \cite[Chapter~1]{stepanov2009elements}.

\subsection{Abstract Model}

A DBM is a statistical model so we may specify by its likelihood
function. Equivalently, it may be viewed as a deep learning method
which is specified by layers and connections between layers.  In the
fully observed setting (i.e. no marginalization over the hidden
layers) the DBM is an exponential family model with a special layer-based
structure.  An exponential family model type, in general, is expressed in
terms of its sufficient statistics (feature maps) and the domains of
its parameter vector.  The likelihood function may be written as
\begin{equation}\label{eq:exponential-likelihood-function}
\mathbb{P}(\overline{\mathbf{u}}) = \frac{h(\overline{\mathbf{u}})}{\mathcal{Z}(\overline{\boldsymbol{\psi}})}\exp( \overline{\phi}(\overline{\mathbf{u}})^\top \overline{\boldsymbol{\psi}})
\end{equation}
where $h(\overline{\mathbf{u}})$ is a fixed function of the inputs
denoting their range: we use this to enforce the binary layers to be
binary, soft-max layers to be integers belonging to a finite set of
categories $[D]$, etc. and we will omit it for the rest of the
presentation and appropriately constrain the domains of the variables.
$\mathcal{Z}$ is the partition function and is equal to
\begin{equation}
\mathcal{Z}(\overline{\boldsymbol{\psi}}) = \exp( \overline{\boldsymbol{\phi}}(\overline{\mathbf{u}})^\top \overline{\boldsymbol{\psi}}).
\end{equation}

The sufficient statistics or feature map is specified in terms of
mutually disjoint groups of variables called the {\it layers} of the
DBM. Supposing there are a total of $J$ layers we specify this complete set as
\begin{equation}
\mathcal{L}=\left\{\mathbf{u}^{(0)},\mathbf{u}^{(1)},\ldots,\mathbf{u}^{(J-1)}\right\}
\end{equation}
so that if $\overline{\mathbf{u}}$ is the complete vector of variables then it is the concatenation
of the layers in $\mathcal{L}$.  The {\it state} of a layer $l$ is the vector of variables
$\mathbf{u}^{(l)}$ hence the states of visible layers correspond to network inputs and outputs whereas
the states of hidden layers are the values of latent variables.  We denote the number of variables in layer $j$ by 
$F_j$. The layers considered in this writeup
are vectors of binary variables (for the hidden layers) which will be written $\mathbf{h}^{(j)}$, 
a vector with a single entry that is a categorical random variable (for a soft-max output layer) which are denoted by $\mathbf{c}^{(j)}$,
and a vector of real-valued variables (for a Gaussian input layer) which are written $\mathbf{v}^{(j)}$.

$\overline{\boldsymbol{\phi}}(\overline{\mathbf{u}})$ denotes the complete set of sufficient statistics
for the complete vector of variables.  The function $\overline{\boldsymbol{\phi}}$ is the concatenation of 
layer bias functions which operate on single layers and layer connection functions which operate on pairs of layers.
The layer bias functions all belong to a set
\begin{equation}
\mathcal{B} = \left\{\boldsymbol{\phi}^{(0)},\ldots,\boldsymbol{\phi}^{(J-1)}\right\}
\end{equation}
where $\boldsymbol{\phi}^{(j)}$ is applied on to the specific variables in its layer $\mathbf{u}^{(j)}$ for
$0\leq j<J$. Each layer bias function $\boldsymbol{\phi}^{(j)}$ is homogeneous: i.e. it concatenates the outputs of a single vector
function $\boldsymbol{\varphi}^{(j)}$
applied to each individual variable $u^{(j)}_d$ of the layer:
\begin{equation}
\boldsymbol{\phi}^{(j)}\left(\begin{bmatrix}u^{(j)}_0 \\ u^{(j)}_1 \\ \vdots \\ u^{(j)}_{F_j}\end{bmatrix}\right) = 
\begin{bmatrix} \boldsymbol{\varphi}^{(j)}(u^{(j)}_0) \\ \boldsymbol{\varphi}^{(j)}(u^{(j)}_1) \\ \vdots \\ \boldsymbol{\varphi}^{(j)}(u^{(j)}_{F_j})\end{bmatrix}.
\end{equation}  The codomain dimension of $\boldsymbol{\phi}^{(j)}$ is denoted by $D_j$.
The particular sufficient statistics featured in the bias layer functions depends on the type of layer.  For binary layers
the sufficient statistics are the coordinates themselves so $\boldsymbol{\phi}^{(j)}(h^{(j)}_d)=h^{(j)}_d\in \{0,1\}^{F_j}=\{0,1\}^{D_j}$;
for soft-max layers the output is a one-hot vector indicating the categorical value:
\begin{equation}
\boldsymbol{\phi}^{(j)}(c^{(j)})=\mathbf{e}_{c^{(j)}}\in \{0,1\}^{D_j}
\end{equation} 
where $\mathbf{e}_c$ is the standard unit vector where the $c$th coordinate is one, the rest are zeros, and the dimension,
$D_j$, is equal to the number of possible categories in the soft-max output;
and, for Gaussian layers the output is the coordinate
and its square: 
\begin{equation}
\boldsymbol{\varphi}^{(j)}(v^{(j)}_i)=\begin{bmatrix} v^{(j)}_i \\ (v^{(j)}_i)^2  \end{bmatrix}
\end{equation}
so that $D_j=2F_j$ and
\begin{equation}
\boldsymbol{\phi}^{(j)}(v^{(j)}) = \begin{bmatrix} \boldsymbol{\varphi}^{(j)}(v^{(j)}_0) \\ \boldsymbol{\varphi}^{(j)}(v^{(j)}_1) \\ \cdots \\ \boldsymbol{\varphi}^{(j)}(v^{(j)}_{F_j}) \end{bmatrix}.
\end{equation}

The layer bias functions operate on a single variable in a single layer wheras the layer connection feature maps operate on two coordinates
in two different layers.  We denote the set of layer connection functions by
\begin{equation}
\mathcal{C} = \left\{ \boldsymbol{\phi}^{\{l,j\}} \mid \{l,j\} \in\mathcal{E}\right\}
\end{equation}
where $\mathcal{E}\subset { [J] \choose 2 }$ is the set of layer edges denoting the connection structure between layers.  In the
context of general multimodal DBMs the $\mathcal{E}$ edges describe an undirected tree graph where visible layers are leaves (although
not all leaves need to be visible).  Each layer connection function or layer edge feature map $\boldsymbol{\phi}^{\{l,j\}}$ is a complete
layer partite function which computes a tensor product between two layers:
\begin{equation}
\boldsymbol{\phi}^{\{l,j\}}(\mathbf{u}^{(l)},\mathbf{u}^{(j)}) = \begin{bmatrix} u^{(l)}_i u^{(j)}_d \end{bmatrix}_{0\leq i < F_l,0\leq d < F_j}.
\end{equation}
These feature maps are the same for binary layers, soft-max layers,
and Gaussian layers.  Observe that these
connection functions are complete, undirected layer-partite functions
that is they are binary, homogeneous, symmetric, and operate only
across layers rather than within a single layer.

\subsubsection{Parameterization of the DBM}

The parameter vector $\overline{\boldsymbol{\psi}}$ is a vector of
real-valued parameters of the same dimension as
$\overline{\mathbf{u}}$.  For each layer feature map (for single
layers or layer connections) $\boldsymbol{\phi}$ with codomain
dimension $D_j$ we have a parameter vectors $\boldsymbol{\psi}$ of
dimension $D_j$ which are the weights in the likelihood function from
\autoref{eq:exponential-likelihood-function}.  The energy function for the distribution can be expanded as follows:
\begin{equation}
\overline{\boldsymbol{\phi}}(\overline{\mathbf{u}})^\top \overline{\boldsymbol{\psi}} = \sum_{j=0}^{J-1} \boldsymbol{\phi}^{(j)}(\mathbf{u}^{(j)})^\top \boldsymbol{\psi}^{(j)} + \sum_{\{l,k\}\in\mathcal{E}} \boldsymbol{\phi}^{\{l,k\}}(\mathbf{u}^{\{l,k\}})^\top \boldsymbol{\psi}^{\{l,k\}}.
\end{equation}
Note that in the Boltzmann Machine literature the inner product $\boldsymbol{\phi}^{\{l,k\}}(\mathbf{u}^{\{l,k\}})^\top \boldsymbol{\psi}^{\{l,k\}}$
is usually written as a bilinear form
\begin{equation}
(\mathbf{u}^{(l)})^\top \mathtt{W}^{(l,k)} \mathbf{u}^{(k)}
\end{equation}
where $\mathtt{W}^{\{l,k\}}$ is a matrix with the same entries as $\boldsymbol{\phi}^{\{l,k\}}$. The domain of
layer edge parameters $\mathtt{W}^{\{l,k\}}$ is the set of real matrices $\mathbb{R}^{F_l\times F_k}$.  

The parameter domains for the
layer bias parameters, however, depend on the layer type.  Binary layer bias parameter vector $\boldsymbol{\psi}^{(j)}\in\mathbb{R}^{F_j}$
are unconstrained real vectors.  Gaussian layers have a more constrained parameterization and by convention their parameters
are written to be suggestive of the Gaussian distribution.  The Gaussian layer bias feature map for layer $j$ 
concatenates the input variable values
$\mathbf{v}^{(j)}$ with a vector of their element-wise squared values $(\mathbf{v}^{(j)})^2$: we call the former the linear features
and the latter the quadratic features.  The parameters in $\boldsymbol{\psi}^{(j)}$ for the quadratic features are constrained to be positive
in $\mathbb{R}^{F_j}_{>0}$ and usually we write them as twice the elementwise square inverse of a vector $\boldsymbol{\sigma}^{(j)}$ so that
$\psi^{(j,quadratic)}_d = 2(\sigma^{(j)}_d)^{-2}$. This convention is adopted so that the square of the diagonal matrix $\diag\boldsymbol{\sigma}^{(j)}$ is the covariance of the Gaussian layer variables conditioned on its parent hidden layer. Keeping with the Gaussian conventions we write
the linear parameter vector as $\psi^{(j,linear)} = \mathbf{b}^{(j)}/(2\sigma^{(j)})$ where the division is taken element-wise and $\mathbf{b}^{(j)}\in\mathbb{R}^{F_j}$.  A Gaussian visible layer $\mathbf{v}$ with its parent binary hidden layer $\mathbf{h}$ (conditioned on all other layers) has
an energy function:
\begin{equation}
  \sum_{i=0}^{F_j-1} \frac{(v_i - b_i)^2}{2\sigma_i^2} - \sum_{i=0}^{F-1}\sum_{d=0}^{F'-1} \frac{v_i}{\sigma_i}\mathtt{W}_{id}h_d - \sum_{d=0}^{F'-1} a_d h_d
\end{equation}
where $F$ is the dimension of the Gaussian layer and $F'$ is the dimension of the parent binary hidden layer. Note that we have also included
the standard deviation vector $\boldsymbol{\sigma}$ in the parameterization of the pairwise connection matrix $\mathtt{W}$.

Soft-max layers or categorical layers also have a constrained
parameterization.  If layer $j$ is a soft-max layer then the observed
variable is categorical and may be written $c^{(j)}\in [D_j]$.  The
sufficient statistics are a one-hot vector (i.e. a member of the
standard coordinate basis) so that entry $c^{(j)}$ of
$\boldsymbol{\phi}^{(j)}(c^{(j)})$ is one and all other entries are
zero.  The parameter vector $\boldsymbol{\psi}^{(j)}\in
\Delta_{D_j-1}$ is a categorical distribution over the $D_j$
categories so the entries are all positive and sum to one.  The energy
function for a categorical visible layer $c$ and a binary hidden layer
$\mathbf{h}$ (conditioned on all other layers) may be written
\begin{align}\label{eq:categorical-hidden-dbm}
- \sum_{i=0}^{D-1}\sum_{k=0}^{F-1}\mathtt{W}_{i,k} \phi(c)_i h_k& - \sum_{i=0}^{D-1} b_i \phi(c)_i  - \sum_{k=0}^{F-1} a_k h_k \notag\\
=& - \sum_{k=0}^{F-1} W_{c,k}h_k - b_c - \sum_{k=0}^{F-1} a_k h_k
\end{align}
since $\phi(c)$ behaves like an indicator vector, and we parameterize the energy function with $\mathtt{W}$, $\mathbf{a}$, and $\mathbf{b}$.


\subsection{Programmatic Model}

Our reference implementation is based on \cite{srivastava2014deepnet} which offers a Python and CUDA-based implementation which we
henceforth refer to as \texttt{deepnet}. \texttt{deepnet} is written in Python and uses two sorts of objects to represent DBMs, the first is
an interface derived from Google's protocol buffers that is primarily used for serialization and, hence, for IO of a DBM specification 
and parameter set (e.g. saving and loading a model).  The second object is a custom object \texttt{deepnet.dbm.DBM} which is
inherited from a \texttt{deepnet.neuralnet.NeuralNet} object. Our discussion of the model will be using the framework provided in
\cite{stepanov2009elements} for discussing concepts which define the abstract properties of object types. 



\section{Formal Model Procedures}

We focus on a multimodal DBM \cite{JMLR:v15:srivastava14b} with a
Gaussian visible layer $\mathbf{v}$, a soft-max label layer $c$, and
binary hidden layers $\overline{\mathbf{h}}$. We strive to keep our
derivations sufficiently general that they may be applied to other
network architectures.  We will do this by using the exponential family
formulation of the DBM likelihood as given by \autoref{eq:exponential-likelihood-function}.

With that form of the likelihood function we
can now derive our maximum-likelihood-based procedures for inference,
estimation, and feature-mapping.  Inference is a procedure defined for
a given DBM.  It maps a vector observation $\mathbf{v}$ and the
current DBM state (i.e. the parameters) to a distribution over
categories which we call the output distribution.  The space of
possible output distributions depends on the loss function and how the
inference procedure is used.  In some contexts we want best guess
inference where we constrain the output distribution to be a one-hot
vector (e.g. for loss functions with zero-one loss).  In other
contexts, such as in a speech recognizer, the ouput distribution is
fed into another learning model and a continuous probability
distribution over categories is appropriate so we use distributional inference.  We will develop the mathematical
theory of both types.

For best guess inference we approximately solve
\begin{equation}\label{eq:best-guess-inference-program}
\arg\max_c \sum_{\overline{\mathbf{h}}} \mathbb{P}(c,\overline{\mathbf{h}}\mid \mathbf{v} ;\overline{\boldsymbol{\psi}})
\end{equation}
for observed data $\mathbf{v}$ and fixed parameters
$\overline{\boldsymbol{\psi}}$.  Unfortunately, \autoref{eq:best-guess-inference-program}
is intractable \cite{chandrasekaran2012complexity} so we opt for solution based on variational bounds.
Moving to the log-domain and applying Jensen's inequality we have
\begin{align}
\log \sum_{\overline{\mathbf{h}}} \mathbb{P}(c,\overline{\mathbf{h}}\mid \mathbf{v};\overline{\boldsymbol{\psi}}) 
&= \log \sum_{\overline{\mathbf{h}}} \mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})
\frac{\mathbb{P}(c,\overline{\mathbf{h}}\mid \mathbf{v};\overline{\boldsymbol{\psi}})}{\mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})} \notag\\
&\geq \sum_{\overline{\mathbf{h}}} \mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})
\log \frac{\mathbb{P}(c,\overline{\mathbf{h}}\mid \mathbf{v};\overline{\boldsymbol{\psi}})}{\mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})}\notag\\
&= \sum_{\overline{\mathbf{h}}} \mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})
\log \mathbb{P}(c,\overline{\mathbf{h}}\mid \mathbf{v};\overline{\boldsymbol{\psi}}) + \mathcal{H}(\mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}}))
\end{align}
where $\mathbb{Q}$ is the factorial distribution over the hidden layers of the DBM, $\overline{\boldsymbol{\mu}}$ is the parameter vector
for $\mathbb{Q}$, and $\mathcal{H}$ denotes the entropy.  When $\mathbb{P}=\mathbb{Q}$ the inequality is tight.  Using this bound we may
write an approximate formulation of \autoref{eq:best-guess-inference-program} as
\begin{equation}\label{eq:best-guess-mean-field-1}
\max_c \max_{\overline{\boldsymbol{\mu}}\in (0,1)^{\dim \overline{\mathbf{h}}}} \sum_{\overline{\mathbf{h}}} \mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})
\log \mathbb{P}(c,\overline{\mathbf{h}}\mid \mathbf{v};\overline{\boldsymbol{\psi}}) + \mathcal{H}(\mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})).
\end{equation}

We may simplify the expression in \autoref{eq:best-guess-mean-field-1} considerably using the exponential family form of the DBM.
Observe
\begin{align}
\log \mathbb{P}(c,\overline{\mathbf{h}}\mid \mathbf{v};\overline{\boldsymbol{\psi}}) =& 
\log \mathbb{P}(c,\overline{\mathbf{h}}\mid \mathbf{v};\overline{\boldsymbol{\psi}}) 
- \log \sum_{c',\overline{\mathbf{h}}'}\mathbb{P}(c',\overline{\mathbf{h}}',\mathbf{v};\overline{\boldsymbol{\psi}})\notag\\
=& \left(\overline{\boldsymbol{\phi}}^{(c,\overline{h})}(c,\overline{\mathbf{h}})\right)^\top \overline{\boldsymbol{\psi}}^{(c,\overline{h})} 
+ \left(\overline{\boldsymbol{\phi}}^{(v)}(\mathbf{v})\right)^\top \overline{\boldsymbol{\psi}}^{(v)} - A(\overline{\boldsymbol{\psi}})
-\log \sum_{c',\overline{\mathbf{h}}'}\mathbb{P}(c,\overline{\mathbf{h}}',\mathbf{v};\overline{\boldsymbol{\psi}})
\end{align}
and observe that the only term depending on $c$ and $\overline{\mathbf{h}}$ is
\begin{equation}
\left(\overline{\boldsymbol{\phi}}^{(c,\overline{h})}(c,\overline{\mathbf{h}})\right)^\top \overline{\boldsymbol{\psi}}^{(c,\overline{h})}
\end{equation}
so solving \autoref{eq:best-guess-mean-field-1} is equivalent to solving
\begin{equation}\label{eq:best-guess-mean-field-2}
\max_c \max_{\overline{\boldsymbol{\mu}}\in (0,1)^{\dim \overline{\mathbf{h}}}} \sum_{\overline{\mathbf{h}}} \mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})
\left(\overline{\boldsymbol{\phi}}^{(c,\overline{h})}(c,\overline{\mathbf{h}})\right)^\top \overline{\boldsymbol{\psi}}^{(c,\overline{h})} + \mathcal{H}(\mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})).
\end{equation}
Since input/output layers such as the categorical softmax layer are a
leaf node within the DBM graph it has a connection to a single hidden layer so the 
energy may be written
\begin{equation}
\left(\boldsymbol{\phi}^{(c)}\right)^\top \boldsymbol{\psi}^{(c)} + \left(\boldsymbol{\phi}^{(c)}\right)^\top W^{(c,h,l)} \mathbf{h}^{(l)} +   \left(\overline{\boldsymbol{\phi}}^{(\overline{h})}(\overline{\mathbf{h}})\right)^\top \overline{\boldsymbol{\psi}}^{(\overline{h})}.
\end{equation}
For a fixed categorical state $c$ we write
\begin{equation}
\left(\overline{\boldsymbol{\phi}}^{(\overline{h})}(\overline{\mathbf{h}})\right)^\top \overline{\boldsymbol{\psi}}^{(\overline{h})}_c = \left(\boldsymbol{\phi}^{(c)}\right)^\top W^{(c,h,l)} \mathbf{h}^{(l)} +   \left(\overline{\boldsymbol{\phi}}^{(\overline{h})}(\overline{\mathbf{h}})\right)^\top \overline{\boldsymbol{\psi}}^{(\overline{h})}
\end{equation}
which emphasizes the limited dependence on the softmax layer state.  Using this new notation the problem is
\begin{equation}\label{eq:best-guess-mean-field-3}
\max_c \left(\boldsymbol{\phi}^{(c)}\right)^\top \boldsymbol{\psi}^{(c)} + \max_{\overline{\boldsymbol{\mu}}\in (0,1)^{\dim \overline{\mathbf{h}}}} \sum_{\overline{\mathbf{h}}} \mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})
\overline{\mathbf{h}}^\top \overline{\boldsymbol{\psi}}^{(\overline{h})}_c
 + \mathcal{H}(\mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})).
\end{equation}
since the feature map $\overline{\boldsymbol{\phi}}^{(\overline{h})}$
is just the identity vector function for hidden binary layers.

We can simplify this expression further.
For a given variable index $i$ in $\overline{\mathbf{h}}$ we have
\begin{align}\label{eq:factorial-exponential-family-cross-entropy-simplification}
\sum_{\overline{\mathbf{h}}}\mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})
\overline{\mathbf{h}}^\top \overline{\boldsymbol{\psi}}^{(\overline{h})}_c =&
  \mu_i \left(\sum_{\overline{\mathbf{h}}_{-i}} \overline{\psi}^{(\overline{h})}_{c,i} + \overline{\mathbf{h}}^\top_{-i} \overline{\boldsymbol{\psi}}^{(\overline{h})}_{-i,c} \right)\notag\\
& + (1-\mu_i) \sum_{\overline{\mathbf{h}}_{-i}} \overline{\mathbf{h}}^\top_{-i} \overline{\boldsymbol{\psi}}^{(\overline{h})}_{-i,c} \notag\\
=& \mu_i \overline{\psi}^{(\overline{h})}_{c,i} + \sum_{\overline{\mathbf{h}}_{-i}} \overline{\mathbf{h}}^\top_{-i} \overline{\boldsymbol{\psi}}^{(\overline{h})}_{-i,c}.
\end{align}
Using mathematical induction and \autoref{eq:factorial-exponential-family-cross-entropy-simplification} we show
\begin{equation}\label{eq:mean-field-energy}
\mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})
\overline{\mathbf{h}}^\top \overline{\boldsymbol{\psi}}^{(\overline{h})}_c = \overline{\boldsymbol{\mu}}^\top \overline{\boldsymbol{\psi}}^{(\overline{h})}_c
\end{equation}
showing that the factorial distribution acts as an approximate continuous encoding of the binary hidden state.  If $\mathcal{E}_{\overline{h}}$
denotes the edge structure for the hidden layers then we may write the hidden layer mean-field cross entropy as
\begin{equation}\label{eq:mean-field-cross-entropy-summation}
\sum_{\{l,k\}\in\mathcal{E}_{\overline{h}}} -\left(\sum_{i,j} \mathtt{W}_{i,j}^{\{l,k\}}\mu^{(l)}_i\mu^{(k)}_j + \sum_{j=0}^{J-1} \sum_i b_{i}^{(j)}\mu_i^{(j)} + \sum_i b_{c,i}^{(J-1)} \mu_i^{(J-1)}\right)
\end{equation}
where $\mathtt{W}^{\{l,k\}}$ is the connection matrix between layers
$l$ and $k$, $\mathbf{b}^{(j)}$ is the bias for layer $j$.  A
subscript $c$ is included in the parameter indices to emphasize that
these are the parameters corresponding to a particular category.
Those weights are produced by the connection between the categorical
layer and the connected binary layer which we denote by
$\mathbf{h}^{(J-1)}$. Note that
\autoref{eq:mean-field-cross-entropy-summation} is linear in the
individual terms of $\overline{\boldsymbol{\mu}}$.

We can apply a similar argument as above to understand the entropy term $\mathcal{H}(\mathbb{Q})$. Observe that
\begin{align}
\mathcal{H}(\mathbb{Q}(\overline{\boldsymbol{\mu}})) 
=& - \sum_{\overline{\mathbf{h}}'}\mathbb{Q}(\overline{\mathbf{h}}';\overline{\boldsymbol{\mu}})\log  \mathbb{Q}(\overline{\mathbf{h}}';\overline{\boldsymbol{\mu}})\notag\\
=& - \sum_{\overline{\mathbf{h}}_{-i}'} (1-\mu_i)\mathbb{Q}(\overline{\mathbf{h}}';\overline{\boldsymbol{\mu}})\left(\log  \mathbb{Q}(\overline{\mathbf{h}}';\overline{\boldsymbol{\mu}}) + \log (1- \mu_i)\right)\notag\\
&- \sum_{\overline{\mathbf{h}}_{-i}'}\mu_i \mathbb{Q}(\overline{\mathbf{h}}';\overline{\boldsymbol{\mu}})\left(\log  \mathbb{Q}(\overline{\mathbf{h}}';\overline{\boldsymbol{\mu}}) + \log\mu_i\right)\notag\\
&= -\sum_{\overline{\mathbf{h}}_{-i}'}\mathbb{Q}(\overline{\mathbf{h}}';\overline{\boldsymbol{\mu}})\log  \mathbb{Q}(\overline{\mathbf{h}}';\overline{\boldsymbol{\mu}})\notag\\
- \mu_i\log\mu_i - (1-\mu_i)\log(1-\mu_i)
\end{align}
so, by induction, we get
\begin{equation}\label{eq:mean-field-entropy}
\mathcal{H}(\mathbb{Q}(\overline{\boldsymbol{\mu}}))  = -\sum_i \left(\mu_i\log \mu_i + (1-\mu_i)\log(1-\mu_i)\right)
\end{equation}
where the index $i$ ranges over all the hidden variables.  Combining \autoref{eq:mean-field-entropy} with \autoref{eq:mean-field-cross-entropy-summation}
our optimization problem for mean-field inference (given a fixed category $c$) is
\begin{align}\label{eq:simplified-category-mean-field-inference}
\max_{\overline{\boldsymbol{\mu}}}-\left(\sum_{\{l,k\}\in\mathcal{E}_{\overline{h}}} \sum_{i,j} \mathtt{W}_{i,j}^{\{l,k\}}\mu^{(l)}_i\mu^{(k)}_j &+ \sum_{j=0}^{J-1} \sum_i b_{i}^{(j)}\mu_i^{(j)} + \sum_i b_{c,i}^{(J-1)} \mu_i^{(J-1)}\right)\notag\\
& - \sum_i \left(\mu_i\log \mu_i + (1-\mu_i)\log(1-\mu_i)\right).
\end{align}

To solve \autoref{eq:simplified-category-mean-field-inference} we derive fixed-point locations for a stationary point of the objective function. 
Defining $L(\overline{\boldsymbol{\mu}})$ to be objective function from \autoref{eq:simplified-category-mean-field-inference}
we get the following stationarity conditions from the partial derivatives:
\begin{align}
0 =& \frac{\partial L(\overline{\boldsymbol{\mu}})}{\partial\mu_i^{(l)}} \notag\\
=& - \left(\sum_{k\in \mathcal{N}_{\mathcal{E}}(j)} \sum_j \mathtt{W}_{i,j}^{\{l,k\}} \mu_j^{(k)} + b_i^{(l)} + \chi(l=J-1)b_{c,i}^{(J-1)}\right)\notag\\
& - \log \frac{\mu_i^{(l)}}{1-\mu_i^{(l)}}
\end{align}
where $\mathcal{N}_{\mathcal{E}}(j)$ is the set of neighboring layers to $j$ in the graph.  We may then derive
\begin{equation}\label{eq:mean-field-fixed-point-coordinate-equation}
\mu_i^{(l)} = \sigma\left(-\left[\sum_{k\in \mathcal{N}_{\mathcal{E}}(j)} \sum_j \mathtt{W}_{i,j}^{\{l,k\}} \mu_j^{(k)} + b_i^{(l)} + \chi(l=J-1)b_{c,i}^{(J-1)}\right]\right)
\end{equation}
where $\sigma(x) = 1/(1+e^{-x})$ is the sigmoid function, i.e. the inverse of the log-odds ratio function. We update the coordinates of
the mean-field layer state $\boldsymbol{\mu}^{(l)}$ using \autoref{eq:mean-field-fixed-point-coordinate-equation}.  We visit each layer
of the network and update the mean-field states using the neighboring layers.

We perform mean-field inference for each category separately since they will have
distinct top-layer states.  Having solved run the equations to convergence we have $\overline{\boldsymbol{\mu}}_{*,c}$ for each category
$c$ and best-guess inference can be performed with a simple search over a finite set where the values are given by
\begin{equation}\label{eq:simplest-best-guess-mean-field-inference}
\max_c \left(\boldsymbol{\phi}^{(c)}\right)^\top \boldsymbol{\psi}^{(c)} + \sum_i b_{c,i}^{(J-1)} \mu_{*,c,i}^{(J-1)}.
\end{equation}

\subsection{Category Distribution Inference}

Distributional inference estimates a distribution over the softmax categories $c$ rather than inferring a single best one.  The natural
choice in an information-theoretic sense is
\begin{align}\label{eq:distributional-inference-variational-inequality}
\log \mathbb{P}(c \mid v; \overline{\mathbf{\psi}}) =& \log\sum_{\overline{\mathbf{h}}}\mathbb{P}(c, \mathbf{h} \mid v; \overline{\mathbf{\psi}})\notag\\
\geq&  \max_{\overline{\boldsymbol{\mu}}}\sum_{\overline{\mathbf{h}}} \mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})\log \mathbb{P}(c, \mathbf{h} \mid v; \overline{\mathbf{\psi}}) + \mathcal{H}(\mathbb{Q}(\cdot;\overline{\boldsymbol{\mu}}))
\end{align}
using a similar variational argument as before and the optimization problem is identical to what we encountered in best-guess inference.
The value of 
\begin{equation}\label{eq:distributional-inference-variational-value}
\max_{\overline{\boldsymbol{\mu}}}\sum_{\overline{\mathbf{h}}} \mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})\log \mathbb{P}(c, \mathbf{h} \mid v; \overline{\mathbf{\psi}}) + \mathcal{H}(\mathbb{Q}(\cdot;\overline{\boldsymbol{\mu}}))
\end{equation}
is equal to the objective function value of \autoref{eq:simplest-best-guess-mean-field-inference} for a given category $c$ up to a constant
factor.  Since \autoref{eq:distributional-inference-variational-value} is an approximation to a log-probability the constant factor $\eta(\mathbf{v};\overline{\boldsymbol{\psi}})$
must ensure proper normalization to probability distribution:
\begin{equation}\label{eq:distributional-inference-normalization-constant}
\eta(\mathbf{v};\overline{\boldsymbol{\psi}}) = -\log\left[\sum_{c'} \exp\left(\left(\boldsymbol{\phi}^{(c)}\right)^\top \boldsymbol{\psi}^{(c)} + \sum_i b_{c,i}^{(J-1)} \mu_{*,c,i}^{(J-1)}.\right)\right].
\end{equation}
As a corollary to our analysis we see that best-guess inference corresponds to taking the mode of the distributional inference output.


 Thus, the best-guess
inference problem becomes
\begin{equation}\label{eq:best-guess-mean-field-4}
\max_c \left(\boldsymbol{\phi}^{(c)}\right)^\top \boldsymbol{\psi}^{(c)} + \max_{\overline{\boldsymbol{\mu}}\in (0,1)^{\dim \overline{\mathbf{h}}}} \overline{\boldsymbol{\mu}}^\top \overline{\boldsymbol{\psi}}^{(\overline{h})}_c
 + \mathcal{H}(\mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})).
\end{equation}

\subsection{Fixed Point Equations}

We use \autoref{eq:best-guess-mean-field-4} to derive a set of fixed point equations to infer the hidden state. For each possible value of
$c$ we solve
\begin{equation}
\max_{\overline{\boldsymbol{\mu}}\in (0,1)^{\dim \overline{\mathbf{h}}}} \overline{\boldsymbol{\mu}}^\top \overline{\boldsymbol{\psi}}^{(\overline{h})}_c
 + \mathcal{H}(\mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})).
\end{equation}
Since
\begin{equation}
\mathcal{H}(\mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})) = - \
\end{equation}


assume a chain graph hidden layers
$\overline{\mathbf{h}}=\left\{\mathbf{h}^{(0)},\ldots,\mathbf{h}^{(J-1)}\right\}$
We may further simplify the expression and use
\autoref{eq:categorical-hidden-dbm} to derive
\begin{align}
\sum_{\overline{\mathbf{h}}} \mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})
\overline{\boldsymbol{\phi}}^{(c,\overline{h})}(c,\overline{\mathbf{h}})^\top \overline{\boldsymbol{\psi}}^{(c,\overline{h})}
=& - b_c - \sum_{\overline{\mathbf{h}}} \mathbb{Q}(\overline{\mathbf{h}};\overline{\boldsymbol{\mu}})\left[  - \sum_{k=0}^{F_0-1} W_{c,k}^{(0)}h_k^{(0)} - \sum_{k=0}^{F_0-1} a_k^{(0)} h_k^{(0)} \right. \notag\\
&\left. 
\qquad\qquad- \left(\sum_{j=1}^{J-1} \left(\mathbf{h}^{(j-1)}\right)^\top W^{(j-1,j)}\mathbf{h}^{(j)} + \left(\mathbf{a}^{(j)}\right)^\top \mathbf{h}^{(j)}  \right)\right]
\end{align}
since the category variable does not appear in the approximating factorial distribution.


Since $\mathbbP{P}(\mathbf{v};\overline{\boldsymbol{\psi}})$ has no dependence on $\mathbb{Q}$ or $c$ we may rewrite




\subsection{A simple Binary network}

In a simple binary network we have a visible binary layer $\mathbf{v}\in\{0,1\}^{D_0}$ and a hidden binary layer
$\mathbf{h}\in\{0,1\}^{D_1}$ with energy function
\begin{equation}
E(\mathbf{v},\mathbf{u};\boldsymbol{\theta},W) = -\mathbf{v}^\top \boldsymbol{\theta}_v - \mathbf{u}^\top \boldsymbol{\theta}_u - \mathbf{v}^\top W \mathbf{u}.
\end{equation}

\subsection{The Gaussian-Categorical Network}

Another interesting case are DBMs with a Gaussian visible layer and a categorical visible layer.  Such DBMs may be used in classication or as an emission distribution of an HMM. Let $\mathbf{v}_0$ denote the input Gaussian variables, $\mathbf{v}_1$ denote the visible categorical
layer, and $\mathbf{h}=(\mathbf{h}_0,\mathbf{h}_1,\mathbf{h}_2)$ the hidden binary layers.  We write the energy function as
\begin{align}
E(\mathbf{v},\mathbf{h}) =& \phi(\mathbf{v}^{(0)})^\top\boldsymbol{\theta}^{(0)} \notag\\
& - (\mathbf{v}^{(0)})^\top W^{(0)} \mathbf{h}^{(0)} - \sum_{i=0,1} (\mathbf{h}^{(i)})^\top W^{(i+1)} \mathbf{h}^{(i+1)}\notag\\
& - (\mathbf{h}^{(2)})^\top W^{(3)} \mathbf{v}^{(1)} - (\mathbf{v}^{(0)})^\top \boldsymbol{\theta}^{(1)}
\end{align}



Each layer is either observed or it is latent
so our specification includes a set of visible layers
\begin{equation}
\mathcal{L}_v=\left\{\mathbf{v}^{(0)},\mathbf{v}^{(1)},\ldots,\mathbf{v}^{\left(N_v\right)}\right\}
\end{equation}
where $N_v$ is the number of visible layers (typically two: one for the inputs and one for the labels).  Each $\mathbf{v}^{(j)}$ for
$1\leq j\leq N_v$ is a vector over some set $\mathbb{F}$ (typically $[2]$, $[k]$ for some $k$, or $\mathbb{R}$) of dimension
$D^{v}_j$ so that $\mathbf{v}^{(j)}\in \mathbb{F}^{D^v_j}$. So the specification of the model must give the dimension or number of variables
in each layer.  Each layer $\mathbf{v}$ will also have a vector of parameters s  

  By convention we will associate visible layers



 It is an example of an undirected graphical model and hence models top-down feedback connections
from higher layers to lower layers.  A multimodal DBM is constructed from a set of visible layers which may have Gaussian or categorical
marginal distributions and a set of hidden layers where each hidden layer is a vector of binary variables.  Many papers have been written
about different data sets that can be used and the inference algorithms have been discussed with algorithms presented at a high level.
In this paper we describe all the details for working with the DBM.  The first topic is to describe inference in the DBM which forms
the basis for the two algorithms we discuss: persistant contrastive divergence and multiprediction training.

We follow the standard recipe for approximate mean-field inference where we infer a distribution over the states of unobserved nodes
using a factorial distribution.  The main contribution of this section is an explicit and programmatic method to perform inference.
The mean-field inference derivation will work out all of the details.  We use a derivation closely related to work 
based on the replicated
soft-max model.

\section{Approximate Inference}

The joint probability distribution over the hidden variables $\mathbf{h}$, over the Gaussian nodes $\mathbf{v}_1$,
and the categorical node $\mathbf{v}_2$ is given using an energy-based function:
\begin{equation}
\mathbb{P}(\mathbf{h},\mathbf{v}_1,\mathbf{v}_2) = \exp(-E(\mathbf{v}_1,\mathbf{v}_2,\mathbf{h});\boldsymbol{\psi})/\mathcal{Z}(\boldsymbol{\psi}).
\end{equation}
Inference is the task of inferring unobserved variables based on the observed variables. One approach is to pick variables that will
maximize the joint likelihood.  We take a different approach using Jensen's inequality and infer factorial distribution that approximates
the conditional distribution of the hidden variables conditioned on the observed variables.
In particular we seek
\begin{align}
\log \sum_{\mathbf{h},\mathbf{v}_2} \mathbb{P}(\mathbf{v}_1,\mathbf{v}_2,\mathbf{h}) =& \log \sum_{\mathbf{h},\mathbf{v}_2} Q(\mathbf{v}_2,\mathbf{h}\mid\mathbf{v}_1) \frac{\mathbb{P}(\mathbf{v}_1,\mathbf{v}_2,\mathbf{h})}{Q(\mathbf{v}_2,\mathbf{h}\mid\mathbf{v}_1)}\\
\geq& \sum_{\mathbf{h},\mathbf{v}_2} Q(\mathbf{v}_2,\mathbf{h}\mid\mathbf{v}_1) \log \frac{\mathbb{P}(\mathbf{v}_1,\mathbf{v}_2,\mathbf{h})}{Q(\mathbf{v}_2,\mathbf{h}\mid\mathbf{v}_1)}
\end{align}
where the inequality is tight in the case where $Q(\mathbf{v}_2,\mathbf{h}\mid\mathbf{v}_1)=\mathbb{P}(\mathbf{v}_2,\mathbf{h}\mid\mathbf{v}_1)$, so we solve the approximate problem
\begin{equation}
\max_{Q\in\mathcal{F}}\sum_{\mathbf{h},\mathbf{v}_2} Q(\mathbf{v}_2,\mathbf{h}\mid\mathbf{v}_1) \log \frac{\mathbb{P}(\mathbf{v}_1,\mathbf{v}_2,\mathbf{h})}{Q(\mathbf{v}_2,\mathbf{h}\mid\mathbf{v}_1)}
\end{equation}
where $\mathcal{F}$ is the set of factorial distributions. This variational problem is the basis for our approximate inference approach.
This variational problem in the context of a DBM leads to fixed point equations called {\it mean-field inference}.  Inference for
categorical variables uses a soft-max function and inference for the hidden binary layers uses a logistic sigmoid.  We do not perform
mean-field inference for the Gaussian variables since they are observed.  Mean-field inference is the process by which we can obtain
predictions in our model.

The update equations are
\begin{align}
\tau_j \leftarrow \frac{1}{2}
\end{align}



Deriving those relationships is of independent interest because it may be seen as performing in-painting.  We want to develop that idea
further because the in-painting may give a sense of which units are accurately modeling the observed data. We want to have an importance
calculated for all of the variables.

\section{Formal Setting}

The DBM is an undirected graphical model with visible nodes $\mathcal{V}$, hidden nodes $\mathcal{H}$, and edge set
$\mathcal{E}$.  Each of the sets of nodes are divided up into layers: $\mathcal{V}=\mathcal{V}_1\sqcup\cdots\sqcup\mathcal{V}_{N_v}$
and $\mathcal{H}=\mathcal{H}_1\sqcup\cdot\sqcup\mathcal{H}_{N_h}$. We define an induced layer graph over the layers where two
layers are said to be adjacent if there is an edge connecting a node in one layer and a node in the other. The layer graph 
in a DBM is an undirected tree and the induced subgraph over two adjacent layers $\mathcal{U},\mathcal{U}'$ is a complete
bipartite graph with parts $\mathcal{U}$ and $\mathcal{U}'$. The probability distribution is defined by a Gibbs distribution
with energy function
\begin{equation}
E(\mathbf{v},\mathbf{h}; W) = \mathbf{v}^\top W_0 \mathbf{h}_1 +\sum_{i=1}^{N_h-1} \mathbf{h}_i^\top W_i \mathbf{h}_{i+1}
\end{equation}  
in the case of a DBM with a single layer of visible nodes and a chain layer graph. The probability distribution over a given
node configuration of the visible nodes is
%% \begin{equation}
%% \mathbb{P}(\mathbf{v}) = \frac{1}{\mathcal{Z}(W)}\sum_{\mathbf{h}}\exp\left(-E(\mathbf{v},\mathbf{h};W)).
%% \end{equation} 

A multimodal DBM has a more complex layer graph structure than a tree.  Each visible layer and sometimes the top hidden layer
are the leaf nodes in the undirected layer tree.  We denote the layer graph tree structure by $\mathcal{E}_L$, 

In the basic case all the variables are binary and the
energy 

 Traditionally, a DBM has a single visible layer $\mathcal{V}_1$
with several hidden layers $\mathcal{H}_1,\ldots,\mathcal{H}_{N_h}$ stacked on top. The subgraph over $\mathcal{V}$ and $\mathcal{H}_1$ 
is a complete bipartite
graph with parts $\mathcal{V}$ and $\mathcal{H}_1$ and the.   The graph structure is defined by an undirected tree over layers. Viewing
the layers as nodes 

We are working in the general setting of multimodal DBMs so the layers are organized in a tree structure where each layer
has a unique parent layer although a given layer may have multiple child layers.  We represent each layer as a vector of variables
with the observed layers denoted as $\mathbf{v}_1,\mathbf{v}_2$ (since we will
at most have two modalities) and the hidden layers as $\mathbf{h}_1,\mathbf{h}_2,\ldots$.  The hidden layers are all binary variables
so that $\mathbf{h}_l\in \{0,1\}^{F_l}$ for $1\leq l\leq K_h$ and the observed layers can be Gaussian, categorical distributions, binary,
or any other type of vector depending on the type of unit. Each observed layer has a complete bipartite connection to a hidden layer
and each hidden layer has complete bipartite connections to any layers below it and a complete bipartite connection to a layer above it.
Thus we may represent the DBM using a graph over the layers where there is a directed edge $\mathcal{E}(\mathbf{u},\mathbf{u}')$
going from a layer $\mathbf{u}$ to its parent layer $\mathbf{u}'$.  Due to the undirected nature of the graph this directed formalism
is merely a topological ordering imposed on the tree-structure over the parts of the graph since a DBM is a $K_h+K_o$-partite graph
where each part only has connections to the neighboring parts and there are no cycles among part connections.

With this formalism we may write the energy function of the distribution in a novel manner:
\begin{equation}
E(\mathbf{v},\mathbf{h};\boldsymbol{\psi}) = \sum_{i=1}^{K_o} E_{o,i}(\mathbf{v}_i) + \sum_{(j,l)\in\mathcal{E}} \mathbf{u}^\top_j W_{j,l}\mathbf{u}_l + \sum_l \mathbf{b}_l
\end{equation}
where $\boldsymbol{\psi}=\{ W_{j,l} \}_{(j,l)\in\mathcal{E}} \cup \{ \mathbf{b}_l \}_{l\in\mathcal{V}}$ represents the connection weights and biases
of each layer, the functions $E_{o,i}$ represents the specific energy to observed unit $i$ (i.e. Gaussian, soft-max, replicated soft-max, rectified-linear, etc.).  The probability distribution over a given configuration of the units is given by the Gibbs distribution
\begin{equation}
\mathbb{P}(\mathbf{v},\mathbf{h};\boldsymbol{\psi}) = \frac{1}{\mathcal{Z}(\boldsymbol{\psi})e^{-E(\mathbf{v},\mathbf{h};\boldsymbol{\psi})}}
\end{equation}
where
\begin{equation}
\mathcal{Z}(\boldsymbol{\psi}) = \int_{\mathbf{v}}\sum_{\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h};\boldsymbol{\psi})}\;d\mathbf{v}
\end{equation}
where the measure $d\mathbf{v}$ is not necessarily continuous with respect to the Lebesgue measure so that we can represent
instances where visible layers are discrete via the integral.

\section{Approximate Inference}



Exact inference over the hidden variables $\mathbf{h}$
conditioned on the visible variables $\mathbf{v}$ in the multimodal DBM is computionally hard so we use approximate inference. Given any distribution $Q(\mathbf{h}\mid\mathbf{v};\boldsymbol{\mu})$ with parameter vector $\boldsymbol{\mu}$
that approximates the true posterior $P(\mathbf{h}\mid\mathbf{v};\boldsymbol{\theta})$ we can compute a variational lower bound
\begin{align}
  \log P(\mathbf{v};\boldsymbol{\theta}) &\geq \sum_{\mathbf{h}}Q(\mathbf{h}\mid \mathbf{v};\boldsymbol{\mu})\log P(\mathbf{v},\mathbf{h};\boldsymbol{\theta}) + \mathcal{H}(Q) \notag\\
 &\geq \log P(\mathbf{v};\boldsymbol{\theta}) - \operatorname{KL}(Q(\mathbf{h}\mid \mathbf{v};\boldsymbol{\mu})\| P(\mathbf{h}\mid \mathbf{v};\boldsymbol{\theta})).
\end{align}
Inference is performed by maximizing
\begin{equation}
\max_Q \sum_{\mathbf{h}}Q(\mathbf{h}\mid \mathbf{v};\boldsymbol{\mu})\log P(\mathbf{v},\mathbf{h};\boldsymbol{\theta}) + \mathcal{H}(Q),
\end{equation}
the lower-bound.  To make the maximization tractable we use a factorial distribution $Q$ where 
\begin{equation}
Q(\mathbf{h}\mid \mathbf{v};\boldsymbol{\mu}) = \prod_{l=1}^{N_h}\prod_{j=1}^{N_l}q(h_j^{(l)}=1\mid \mathbf{v}) = \prod_{l=1}^{N_h}\prod_{j=1}^{N_l}\mu_j^{(l)}
\end{equation}
which will yield fixed-points for inference.

We are most interested in the case where we have a soft-max visible
layer for the layers and a Gaussian input layer.  We have two separate
instances of inference: the first is the case where the actual
posteriors are observed so we are jointly modeling the inputs and the
outputs.  The second case is where the soft-max layer is hidden but we
still observe the Gaussian visible layer $\mathbf{v}_1$ and we wish to predict the
soft-max layer $\mathbf{v}_2$.  We will consider the two problems
separately.

In the case where we observe both the soft-max layer and the Gaussian layer we have a log joint probability distribution:
\begin{align}
\log P(\mathbf{v}_1,\mathbf{v}_2,\mathbf{h};\boldsymbol{\theta}) =& \left(\mathbf{v}^{(1)}\right)^\top \diag(\sigma^{-2})/2 \mathbf{v}^{(1)} - \left(\mathbf{v}^{(1)}\right)^\top W^{(1)} \mathbf{h}^{(1)} \notag\\
&  - \left(\mathbf{v}^{(2)}\right)^\top W \mathbf{h}^{(2)} - \mathbf{h}^{(2)} W^{(2,3)}\mathbf{h}^{(3)}\notag\\
& - \left(\mathbf{h}^{(1)} \right)^\top W^{(1,3)} \mathbf{h}^{(3)}.
\end{align}
To compute the cross entropy between the factorial distribution and the joint using a probability for the factorial distribution
over $\mathbf{v}_2$ (the soft-max layer) where
\begin{equation}
Q(\mathbf{v}^{(2)}_j = 1\mid \mathbf{v}^{(1)};\boldsymbol{\mu}) = \boldsymbol{\mu}^{(v,2)}_j
\end{equation}
where 
\begin{equation}
\sum_j \boldsymbol{\mu}^{(v,2)}_j = 1.
\end{equation}
The entropy of the soft-max layer is
\begin{equation}
\sum_j \frac{\boldsymbol{\mu}^{(v,2)}_j}{\sum_{j'}\boldsymbol{\mu}^{(v,2)}_{j'}} \log \frac{\boldsymbol{\mu}^{(v,2)}_j}{\sum_{j'}\boldsymbol{\mu}^{(v,2)}_{j'}}
\end{equation}
then we call
\begin{align}
\sum_{\mathbf{h}} Q(\mathbf{h}\mid \mathbf{v};\boldsymbol{\mu})\log P(\mathbf{v}_1,\mathbf{v}_2,\mathbf{h};\boldsymbol{\theta}) =&
\end{align}

\subsection{Softmax Unit Inference}




\section{Approximating the objective}

The training objective is to maximize the likelihood over the observed data.  Which we write as
\begin{align}
\mathcal{L}(\boldsymbol{\psi};\mathbf{v}) = \log \sum_{\mathbf{h}} \mathbf{P}(\mathbf{v},\mathbf{h};\boldsymbol{\psi})
\end{align}


\section{MNIST Experiment}

To run this experiment we first construct a number of objects:
We have
\begin{verbatim}
model, train_op, eval_op = LoadExperiment(sys.argv[1], sys.argv[2],
                                          sys.argv[3])
\end{verbatim}
where \texttt{model} is of type \texttt{deepnet\_pb2.Model}, \texttt{train\_op} and
\texttt{eval\_op} is of type \texttt{deepnet\_pb2.Operation}.  We then call
a constructor \texttt{dbm.DBM} on \texttt{model}, \texttt{train\_op}, and \texttt{eval\_op}.

So the initialization just sets up the objects.  The protobuff objects were generated automatically,
I'm not entirely sure how that works (that's a comment at the top of the file).

\subsection{Train}

Those parameters are used to create an instance of \texttt{dbm.DBM} denoted by \texttt{model}
and then we call \texttt{model.Train()} which calls \texttt{model.SetUpTrainer()}.  The first expression
evaluated in \texttt{SetUpTrainer} is a method \texttt{LoadModelOnGPU}.

In \texttt{LoadModeOnGPU} we then run through all the layers and copy hyperparameters from the \texttt{model.pbtxt}
training file into the layer.  After doing that processing we then append the layer
to a list of layers \texttt{dbm.DBM.layer} which then creates an object subclassed from \texttt{layer.Layer}.
The \texttt{layer.Layer} subclass contains many useful pieces of code that indicate various parameters of that layer.
A logical thing would be to make it so that the several layers would have parameters that match the 

\section{Modifying the MNIST experiment}

The setup for the experiment is very similar between the MNIST experiment and the African languages
experiment. Additionally the setup is very similar to the Fisher experiment I considered when testing 
the pylearn2 implementation.  When running the African languages experiment I will need to modify the 
data handling objects in \texttt{deepnet} since they presently cannot handle large datasets.
When running the Fisher experiment I will be able to just modify the \texttt{proto} files
without changing too much about the network probably. Thus, I am first going to run a version of the 
Fisher experiment and then do the modifications for the African languages experiment.

\subsection{Fisher Experiment}

To run this experiment we first need to create a dataset similar to the mnist dataset but with the fisher data.
I just altered the size of the network and such and it appears that I was able to get a very low number.
One question is whether I can get the posteriors out from the network easily and whether I can get features
easily out once I have used a learned network.  That is criticial for running things

\subsection{Adapting the Data Input}

My strategy for figuring out where the data is loaded in is to follow the path of execution
and try to start writing the code to get the data processed properly.  \texttt{neuralnet.NeuralNet.SetUpTrainer}
is a method called by \texttt{neuralnet.NeuralNet.Train} (both of these are in the \texttt{deepnet} module). Many of the
attributes of \texttt{neuralnet.NeuralNet} are defined by the \texttt{eval.pbtxt} and \texttt{train.pbtxt} 
protobuffer files.  After loading in the model using \texttt{neuralnet.NeuralNet.LoadModelOnGPU}
we then run \texttt{neuralnet.NeuralNet.SetUpData}.

Within that function the first step is to keep track of the data names and to get the data handles
for the training, validation, and testing datasets using the training \texttt{deepnet\_pb2.Operation} object
that is often referred to by \texttt{t\_op} and is the \texttt{neuralnet.NeuralNet.t\_op} attribute of the
neural network class.  We then use the function \texttt{GetDataHandles} that returns a list of 
data handles for \texttt{self.train\_data\_handler}, \texttt{self.validation\_data\_handler},
\texttt{self.test\_data\_handler}.

During the run of \texttt{GetDataHandles} we then call the \texttt{datahandler.DataHandler} constructor
over the pairs of input layer data names:
\begin{verbatim}
[[u'train_data', u'train_labels'], [u'validation_data', u'validation_labels'], [u'test_data', u'test_labels']].
\end{verbatim}
Then three \texttt{datahandler} objects are created:
\begin{enumerate}
\item \texttt{datahandler.Disk}
\item \texttt{datahandler.Cache}
\item \texttt{datahandler.GPUCache}
\end{enumerate}
we will go over each of these and address what they do. \texttt{datahandler.Disk} looks like it handles the 
IO from the disk.  Essentially these three classes represent the memory hierarchy between Disk, CPU,
and GPU.  We want to grab data from the disk in chunks (thus allowing us to use hdf5).  Thus we can simply add
into the \texttt{datahandler.Disk.ReadDiskData} method and extend it to handle \texttt{hdf5} datasets
so that we can work with the larger dataset.  The change should allow one to basically loop through the whole
dataset and grab chunks of it at a time.  When you get to the end you should loop back around.  All of that logic
can probably just be handled by the \texttt{datahandler.Disk} class logic itself and does not need to affect the other
classes.

It appears that as a part of the implementation we can use multiple files using wild cards \texttt{*} since the filenames to the
data are resolved with \texttt{glob.glob}.  To see whether I can get this to work I will split the fisher data up into 4 files
one with 30 examples, the next with 30, the third with 30 and the final with 10.  I will be curious whether the \texttt{datahandler.Cache}
is the same no matter how many files I have. So before I do this, I first run a \texttt{pdb.set\_trace()} statement within
\texttt{datahandler.Cache.LoadData} and see that the data has been loaded into the cache when there is  only a single file. 
The next thing to check is what happens when I split the data into multiple files.

After running the check on the Fisher I see that multiple files works just fine.

\subsection{Handling Posteriors as Input}

The next challenge is to get the network to accept posteriors as input since the MNIST network expects plain labels. I'm going to
look in the code for files that could handle a categorical distribution input and see whether that could work.  One thing
to check is the original paper and to check the experiments that were run for the multimodal dbn.

After running it seems there is an implementation of a softmax layer but it is accustomed to getting labels as input.  Thus,
this section of the writeup will be spent understanding exactly how to get the soft-max computations to work with posteriors
as input. I'm going to have to work somewhat with the CUDA materials to be able to get this to happen.

\section{Deepnet Data Flow}

To understand the algorithm working in deepnet we go over the inner workings of the algorithm.  One of the main purposes is
to understand the actual training objectives and what the gradients computed are.  I want to be able to match up everything that
is happening in the code to what is happening in the paper.  To do this I will very carefully run through all the code that gets
executed.

\subsection{Initialization}

The first method called is \texttt{dbm.DBM.\_\_init\_\_} which calls the inherited initialization method
\texttt{neuralnet.NeuralNet.\_\_init\_\_}.  The main things that happen here are the initialization to of all the attributes that define the
network to null values.  Of greatest interest are
\begin{verbatim}
self.data = None
self.edge = []
self.layer = []
\end{verbatim}
the network parameters are stored in \texttt{neuralnet.NeuralNet.net}.  The initialization does not do much more than this.
It also sets the parameters for the evaluation and training operations to \texttt{model.t\_op} and
\texttt{model.e\_op}, respectively.

\subsection{Loading Model Parameters on to the GPU}

The next step is to load the models onto the GPU and set the basic model parameters
\texttt{model.edge} and \texttt{model.layer}.  The first step is to get the batchsize set. Normally this is inferred from the 
parameters in \texttt{model.t\_op} where that was set to be from the \texttt{train.pbtxt} file. The next step is to populate
the layers of the network in \texttt{model.layer} using \texttt{util.CreateLayer} (one point of confusion often occurring while
working with this code is that function names are often in camel text so that you don't always know whether something is a constructor
or not). The hyperparameters from \texttt{model.net.hyperparams} are then copied over to \texttt{layer.hyperparams} using 
the \texttt{MergeFrom} method.  This is an important step as those hyperparameters are the ones set in the model \texttt{.pbtxt}
file that was loaded into the \texttt{trainer.py} script.

The ultimate output of \texttt{dbm.DBM.LoadModelOnGPU()} creates \texttt{dbm.DBM.edge} which contains the parameter matrices
and \texttt{dbm.DBM.layer} which houses the internal states of nodes (either a hidden state or the input data). And this gets us the
defining characteristics of the DBM implementation included in the code.  These are initialized randomly.

Another critical thing in the model is the topological information revealed by the edge structure which organizes
the computation strategy given by the network.  In the case of DBMs we use the \texttt{dbm.DBM.Sort} method which organizes
the nodes in the order that we treat them during data processing.

\subsection{Loading Data Into Program}

Once the model has been setup the next step is to set up the data which primarily uses tools from \texttt{datahandler.py}.
That file has three classes that are of interest.  The \texttt{dbm.DBM} class has a method \texttt{dbm.DBM.SetUpData} which
basically just calls the inherited method \texttt{neuralnet.NeuralNet.SetUpData} unless \texttt{dbm.DBM.initializer\_net} is not
\texttt{None}.  First the program extracts the layers with input and output, gets the data handles according to the data
script as well as the hyperparameters that were set for those layers.

The function \texttt{datahandler.GetDataHandles} is then called and produces three \texttt{datahandler.DataHandler} instances
one for the training data, one for the validation data, and one for the testing data.  Eaach of the \texttt{datahandler.DataHandler}
instances has three objects for managing the memory hierarchy:
\begin{enumerate}
\item \texttt{datahandler.DataHandler.disk} which is an instance of \texttt{datahandler.Disk} and handles loading in data stored on disk. Really the only method that needs to be implemented for this object is \texttt{datahandler.Disk.Get} which loads in data.  The object uses
private methods (private by convention) to actually handle the loading.
\item \texttt{datahandler.DataHandler.cache} which is an instance of \texttt{Cache} which holds a smaller set of data in the CPU.
\item \texttt{datahandler.dataHandler.gpu\_cache} which is an instance of \texttt{GPUCache} which manages the data that is actually loaded
onto the GPU device.
\end{enumerate}
The data itself is refered to via handles and in the case of the MNIST data and in the African data experiments for each of
training, validation, and testing we have a handle for the continuous input data and a handle for the label posteriors data.

\subsection{Loop parameter settings}

The rest of the training setup involves assigning values to the model objects which indicate stopping conditions for training, how often
we should evaluate the error rates, how often the model should be saved, and what criteria should be used to select the best model
via the validation criterion performance.

\subsection{The Training loop}

The training loop was written simply with the following steps:
\begin{enumerate}
\item Load in the training data \texttt{self.GetTrainBatch}
\item Compute the losses and do the update \texttt{self.TrainOnebatch}
\item Run an \texttt{Accumulate(acc,loss)} where \texttt{loss} is the current minibatch losses, and \texttt{acc} is the accumulated losses across several minibatches.
\end{enumerate}

\texttt{GetTrainBatch} just calls \texttt{GetBatch} but with the appropriate handle for the training data (stored in \texttt{model.train\_data\_handler}--there are also similar methods that work analogously: \texttt{GetValidationBatch}, \texttt{GetTestBatch}).  This function
calls the \texttt{handler.Get} method that retrieves a reference the \texttt{CUDAMatrix} object represented a mini-batch of data
on the GPU and sets the states of the network layers in \texttt{dbm.DBM.datalayer} to those matrices.  The \texttt{dbm.DBM.train\_data\_handler}
was setup in the \texttt{SetUpData} section.  The \texttt{db.DBM.trin\_data\_handler.Get} method simply calls the \texttt{datahandler.GPUCache.Get} method which gets a matrix from the GPU.  Getting the matrix of batch data from the GPU works via lazy-loading: if the GPU does not
have the next mini-batch loaded then that call to the \texttt{Get} method will cause the GPU to grab the batch for the \texttt{Cache} which
models the CPU data, and if the CPU cache does not have the data then a \texttt{Get} method is called for the \texttt{dbm.DBM.train\_data\_handler.disk} instance and more data is loaded from the disk.  The \texttt{SetData} method called by the layers stored in the 
\texttt{dbm.DBM.datalayer} list simple pass a name (because Python) to the \texttt{layer.data} attribute for the data layers so that
methods within the layer can now work with the \texttt{CUDAMatrix} instance containing the current mini-batch of data used by the algorithm.

Once all of that data loading has been taken care of the next step is \texttt{dbm.DBM.TrainOneBatch} which takes the \texttt{dbm.DBM}
and the current step number as arguments.  This is the heart of the training algorithm and we explain what is happening there
in the next section.

\subsection{Training Step}

The training algorithm employed is Persistant Contrastive Divergence.  The complete method in the \texttt{dbm.DBM} class is
\begin{verbatim}
  def TrainOneBatch(self, step):
    losses1 = self.PositivePhase(train=True, step=step)
    if step == 0 and self.t_op.optimizer == deepnet_pb2.Operation.PCD:
      self.InitializeNegPhase(to_pos=True)
    losses2 = self.NegativePhase(step, train=True)
    losses1.extend(losses2)
    return losses1
\end{verbatim}

There is a special initialization step that we consider in the case where you are on the first step.  The first part of the algorithm
is the same from step to step, however.  This is the positive phase.

\subsubsection{DBM PCD Positive Phase}

If there is an initializer net then we do a forward propagation with
that (will return to the initializer net later to see if it has
anything to do with pretraining and the like.  The first part of the
positive phase is to initialize each of the layers.  In the code the
layer is written as \texttt{dbm.DBM.node\_list} which is the same in
the case of the DBM being considered, the \texttt{node\_list} contains
the topologically sorted layers for inference purposes. 

For the data layer nodes we call the \texttt{dbm.DBM.ComputeUp} method
which will simply get the data and set the state of the input data layers
to the data matrix (a \texttt{cudamat.CUDAMatrix}).  The rest of the states
are all going to be set to zero.

The next step of the computation is mean-field inference
and we run the number of steps as given by the hyperparameters in the model
\texttt{pbtxt} file (protocol buffer config file).  The \texttt{dbm.DBM.Sort}
method was called earlier and it organizes the layers based on the order they are processed. In the case of
the network given in this writeup there are five layers with two data layers and only the two non data layers
will have mean-field inference run so those three layers are then run.  The mean-field inference is run the usual way:
each layer updates its state by taking the inner product between the edge weights and the state of the neighbour layer,
adds them together, adds the bias, and then takes the sigmoid of that matrix and stores that as the state for the layer.  
A little bit more is done afterwards in the case where dropout rules are in place.

The last step is to collect the sufficient statistics into a
\texttt{cudamat.CUDAMatrix} instance in \texttt{layer.suff\_stats}.
The sufficient statistics will be used in the negative phase. For
layers the sufficient statistics are computed by summing over the data
dimension of the state (since the state is dimensionality on the
zeroeth axis and mini-batch size on the first axis).  The sufficient
statistics for edges are the dot product between the state matrices
between the two layers that the edge joins.

\subsubsection{DBM PCD Negative Phase}

The first step is to set the \texttt{layer.state} and \texttt{layer.sample}
to \texttt{layer.neg\_state} and \texttt{layer.neg\_sample} since each layer carries around a positive and negative version.
The numer of Gibbs sampling steps is then set to the value of the hyperparameters from the protocol buffer model file.
\texttt{dbm.DBM.ComputeUp} is then run on the layers where the inputs to update the state is the \texttt{layer.sample}
attribute since the negative phase is built on estimating an expectation.  Then sampling is done at all the layers:
rectified linear units use Gaussians, softmax layers use a categorical distribution, and logistic layers use Bernoulli
random variables.

Then the sufficient statistics computed in the postive phase are then adjusted by taking the row sums (across) samples
and subtracting the state inferred from the negative phase via sampling.  The gradient at the layer bias term is then given by
\begin{equation}
\partial_t \mathbf{b} <- \partial_{t-1}\mathbf{b} - \mathbf{s}/N_{batch}
\end{equation}
where $\mathbf{s}$ is the vector of sufficient statistics and $\partial_{\tau}\mathbf{b}$ is the estimated gradient at
time instant $\tau$.

A similar update is then done with the edges.  The phase is then set back to the positive phase.

\subsubsection{First Pass Negative Phase Initialization}

In the first run of the program we need to get the \texttt{layer.sample} and \texttt{layer.state} set properly in order
to run the negative phase since those are relied upon for the computations.

\subsection{Making Predictions}

The final part of the code is understanding how the error rates and evaluation occurs in the model. Basically, it involves
just running the positive phase of PCD.  Mean-field inference is then used to get a prediction about what the state should
be of the input data-layer nodes and then that is compared to the true data and the difference between the two is the loss.

\section{Computing the Features}

In order to do computations with the network what matters is the layer state for the layers that accept the input data.
So getting the inferred posteriors for all of the data is simply a matter of loading in the input data in a sensible way.

\section{Accepting Pre-trained network and training without Posterior Labels}

After boot-strapping the training process to train against GMM posterior labels we then move to training.


\bibliography{dbm.bib}
\bibliographystyle{plain}


\end{document}
